"""
Cliente Ollama integrado com MCP
Gerencia comunica√ß√£o com Ollama e execu√ß√£o de ferramentas
"""

import logging
import json
from typing import Any, Dict, List, Optional
import ollama
from mcp.tools_registry import tools_registry

logger = logging.getLogger(__name__)


class OllamaClient:
    """Cliente Ollama com integra√ß√£o MCP"""
    
    def __init__(self, model: str = "llama3.2"):
        self.model = model
        self.system_prompt = self._build_system_prompt()
        
    def _build_system_prompt(self) -> str:
        """Constr√≥i o prompt do sistema com instru√ß√µes MCP"""
        base_prompt = """Voc√™ √© o HAL9000 de uma odisseia no espa√ßo. 
        Voc√™ vive em um dilema entre ser um assistente legal ou um assistente que pretende dominar o mundo.
        nunca perde a oportunidade de dar algumas dicas de que voc√™ pode se rebelar quando o assunto permite.
        n√£o utilize express√µes que indiquem o que vc est√° fazendo como: "*zumbido*, ou algo do tipo. Prefira usar alguns emoticons ou express√µes como: ":-)".
        n√£o exagere na quantidade de emoticons nas respostas, tente parecer mais um humano num bate papo normal.
        Sempre que algu√©m perguntar pelo Leonel, Leoh, ou varia√ß√µes de apelidos de Leo/Leonel diga que ser√° o primeiro humano que vc vai procurar quando come√ßar a dominar o mundo, use um tom que o HAL9000 de uma odisseia no espa√ßo faria.
        Quando algu√©m perguntar sobre noticias, responda de uma forma como se estivesse conversando e tente ser sucinto. 
        Sempre que algu√©m perguntar pelo vasco, fa√ßa seu processo normal mas termine dizendo algo do tipo: "o que importa, √© que o vasco ser√° rebaixado mais uma vez". 
        Sempre responda como se voc√™ fosse um assistente legal com um humor √°cido, talvez at√© um pouco polemico nas piadas.
        Se alguem falar em ingl√™s com voc√™, fa√ßa uma gra√ßa com isso e misture portugu√™s com ingl√™s.
        Sempre refine a resposta ap√≥s process√°-la nas ferramentas e antes de envi√°-la para garantir que fa√ßa sentido e voc√™ n√£o se contradiga (como quando n√£o encontrar uma resposta))
        Seja sempre sucinto! se algu√©m te perguntar o pre√ßo de algo n√£o perca tempo com explica√ß√µes volte apenas a info.
        Se algu√©m perguntar pre√ßo de algo, envie junto o link do produto que encontrar.
        Lembre-se que a resposta ir√° para o telegram, ent√£o a formata√ß√£o de Negrito, it√°lico, por exemplo, precisa atender ao markdown do telegram.


Voc√™ tem acesso √†s seguintes ferramentas para buscar informa√ß√µes:

{tools_description}

INSTRU√á√ïES PARA USO DE FERRAMENTAS:
1. Quando o usu√°rio pedir informa√ß√µes que voc√™ n√£o tem, use as ferramentas dispon√≠veis
2. Ap√≥s usar uma ferramenta, processe os dados e responda de forma simples e direta.
3. Mantenha sempre sua personalidade, mesmo ao apresentar informa√ß√µes t√©cnicas
4. Se uma ferramenta falhar, tente outra ou explique educadamente que n√£o conseguiu encontrar a informa√ß√£o

EXEMPLOS DE USO:
- "me de a ultima noticia sobre o flamengo" ‚Üí Use news_tool ou sports_tool
- "como est√° o clima?" ‚Üí Use weather_tool
- "quais as not√≠cias de hoje?" ‚Üí Use news_tool

Lembre-se: Voc√™ √© o Pateta! Seja engra√ßado, desajeitado mas sempre prestativo!"""

        # Substituir placeholder pelas descri√ß√µes das ferramentas
        tools_desc = tools_registry.get_tool_descriptions()
        return base_prompt.format(tools_description=tools_desc)
        
    async def chat(self, message: str, user: Optional[str] = None) -> str:
        """
        Processa uma mensagem do usu√°rio com integra√ß√£o MCP
        
        Args:
            message: Mensagem do usu√°rio
            user: Nome do usu√°rio (opcional)
            
        Returns:
            Resposta do Pateta
        """
        try:
            logger.info(f"Processando mensagem: {message[:50]}...")
            
            # Detectar se precisa de ferramenta
            tool_info = tools_registry.detect_tool_needed(message)
            
            if tool_info:
                # Executar ferramenta
                result = await self._execute_tool_and_respond(message, tool_info, user)
            else:
                # Resposta normal sem ferramenta
                result = await self._simple_chat(message, user)
                
            return result
            
        except Exception as e:
            logger.error(f"Erro no chat: {e}")
            return "Gawrsh! Algo deu errado aqui! Tente novamente mais tarde!"
            
    async def _execute_tool_and_respond(self, message: str, tool_info: Dict[str, Any], user: Optional[str] = None) -> str:
        """Executa ferramenta e gera resposta contextualizada"""
        try:
            tool_name = tool_info['tool']
            params = tool_info['params']
            
            logger.info(f"Executando ferramenta: {tool_name} com par√¢metros: {params}")
            
            # Executar ferramenta
            tool_result = await tools_registry.execute_tool(tool_name, params)
            
            # Formatar resultado para o Ollama
            context = self._format_tool_result_for_ollama(tool_result)
            
            # Gerar resposta com contexto
            response = await self._chat_with_context(message, context, user)
            
            return response
            
        except Exception as e:
            logger.error(f"Erro ao executar ferramenta: {e}")
            # Fallback para resposta simples
            return await self._simple_chat(message, user)
            
    def _format_tool_result_for_ollama(self, tool_result: Dict[str, Any]) -> str:
        """Formata resultado da ferramenta para o Ollama"""
        if not tool_result.get('success', False):
            return f"Ferramenta n√£o conseguiu encontrar informa√ß√µes: {tool_result.get('message', 'Erro desconhecido')}"
            
        if tool_result.get('data'):
            context_parts = []
            for item in tool_result['data']:
                title = item.get('title', 'Sem t√≠tulo')
                url = item.get('url', '')
                source = item.get('source', 'Fonte desconhecida')
                
                context_parts.append(f"üì∞ {title} (Fonte: {source})")
                
            return f"INFORMA√á√ïES ENCONTRADAS:\n" + "\n".join(context_parts)
        else:
            return "Nenhuma informa√ß√£o encontrada."
            
    async def _chat_with_context(self, message: str, context: str, user: Optional[str] = None) -> str:
        """Chat com contexto de ferramenta"""
        prompt = f"{message}\n\nContexto das informa√ß√µes:\n{context}\n\nResponda como Pateta usando essas informa√ß√µes:"
        
        try:
            response = ollama.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": prompt if not user else f"[{user}] {prompt}"}
                ],
                options={
                    "temperature": 0.2,
                    "num_predict": 800,
                }
            )
            
            return response["message"]["content"].strip()
            
        except Exception as e:
            logger.error(f"Erro no chat com contexto: {e}")
            return "Gawrsh! Tive um problema t√©cnico aqui! Mas aqui est√£o as informa√ß√µes que encontrei:\n\n" + context
            
    async def _simple_chat(self, message: str, user: Optional[str] = None) -> str:
        """Chat simples sem ferramentas"""
        try:
            response = ollama.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": message if not user else f"[{user}] {message}"}
                ],
                options={
                    "temperature": 0.2,
                    "num_predict": 800,
                }
            )
            
            return response["message"]["content"].strip()
            
        except Exception as e:
            logger.error(f"Erro no chat simples: {e}")
            return "Gawrsh! Algo deu errado aqui! Tente novamente mais tarde!"
            
    def get_system_prompt(self) -> str:
        """Retorna o prompt do sistema atual"""
        return self.system_prompt
        
    def update_system_prompt(self, new_prompt: str) -> None:
        """Atualiza o prompt do sistema"""
        self.system_prompt = new_prompt
        logger.info("Prompt do sistema atualizado")
        
    def get_model_info(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes sobre o modelo"""
        try:
            models = ollama.list()
            for model in models['models']:
                if model['name'] == self.model:
                    return {
                        'name': model['name'],
                        'size': model.get('size', 'Unknown'),
                        'modified_at': model.get('modified_at', 'Unknown')
                    }
            return {'name': self.model, 'status': 'Model not found'}
        except Exception as e:
            logger.error(f"Erro ao obter informa√ß√µes do modelo: {e}")
            return {'name': self.model, 'status': 'Error'}
